{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from IPython import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'train-balanced-sarcasm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010826, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010826 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      "label             1010826 non-null int64\n",
      "comment           1010773 non-null object\n",
      "author            1010826 non-null object\n",
      "subreddit         1010826 non-null object\n",
      "score             1010826 non-null int64\n",
      "ups               1010826 non-null int64\n",
      "downs             1010826 non-null int64\n",
      "date              1010826 non-null object\n",
      "created_utc       1010826 non-null object\n",
      "parent_comment    1010826 non-null object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 77.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    505405\n",
       "1    505368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(train_df['comment'].values, train_df['label'].values, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(texts):\n",
    "    return [re.sub(r\"([^ \\w])\", r\" \\1 \", str.lower(text)) for text in texts]\n",
    "\n",
    "def tokenization(texts):\n",
    "    return [text.split() for text in texts]\n",
    "\n",
    "def build_vocabulary(data):\n",
    "    vocab = dict()\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            try:\n",
    "                vocab[w]\n",
    "            except:\n",
    "                vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def build_embeddings_glove(file_path, vocab, d=300):\n",
    "    emb_dict = dict()\n",
    "    unk_array = np.zeros(d)\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vocab[word]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                emb_dict[word] = vector\n",
    "                unk_array += vector\n",
    "            except:\n",
    "                continue\n",
    "    emb_dict['UNK'] = unk_array / len(emb_dict)\n",
    "    return emb_dict\n",
    "\n",
    "def build_w2v_dict(file_path, vocab, d=300):\n",
    "    emb_dict = dict()\n",
    "    unk_array = np.zeros(d)\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            vector = w2v_model.get_vector(word)\n",
    "            emb_dict[word] = vector\n",
    "            unk_array += vector\n",
    "        except:\n",
    "            continue\n",
    "    emb_dict['UNK'] = unk_array / len(emb_dict)\n",
    "    return emb_dict\n",
    "\n",
    "def build_emb_matrix_lr(data, emb_dict):\n",
    "    X = []\n",
    "    cnt_unk = 0\n",
    "    cnt_total = 0\n",
    "    for d in data:\n",
    "        sentence_emb = np.zeros(len(emb_dict['UNK']))\n",
    "        for w in d:\n",
    "            cnt_total += 1\n",
    "            try:\n",
    "                sentence_emb += emb_dict[w]\n",
    "            except:\n",
    "                cnt_unk += 1\n",
    "                sentence_emb += emb_dict['UNK']\n",
    "        X.append(sentence_emb / len(d))\n",
    "    return np.array(X), cnt_unk / cnt_total\n",
    "\n",
    "def build_emb_dict_nn(file_path, vocab, d=300):\n",
    "    emb_dict = dict()\n",
    "    unk_array = np.zeros(d)\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vocab[word]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                emb_dict[word] = vector\n",
    "                unk_array += vector\n",
    "            except:\n",
    "                continue\n",
    "    emb_dict['UNK'] = unk_array / len(emb_dict)\n",
    "    emb_dict['PAD'] = np.zeros(d)\n",
    "    return emb_dict\n",
    "\n",
    "def build_emb_matrix_nn(file_path, vocab, d=300):\n",
    "    emb_dict = build_emb_dict_nn(file_path, vocab, d=d)\n",
    "    emb_matrix = np.zeros((len(emb_dict), d))\n",
    "    word2idx = {'UNK': 0, 'PAD': 1}\n",
    "    for word in sorted(list(set(emb_dict.keys()) - set(['UNK', 'PAD']))):\n",
    "        word2idx[word] = len(word2idx)\n",
    "    for w, i in word2idx.items():\n",
    "        emb_matrix[i] = emb_dict[w]\n",
    "    emb_matrix = torch.tensor(emb_matrix)\n",
    "    return emb_matrix, word2idx\n",
    "\n",
    "def build_w2v_dict_nn(file_path, vocab, d=300):\n",
    "    emb_dict = dict()\n",
    "    unk_array = np.zeros(d)\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            vector = w2v_model.get_vector(word)\n",
    "            emb_dict[word] = vector\n",
    "            unk_array += vector\n",
    "        except:\n",
    "            continue\n",
    "    emb_dict['UNK'] = unk_array / len(emb_dict)\n",
    "    emb_dict['PAD'] = np.zeros(d)\n",
    "    return emb_dict\n",
    "\n",
    "def build_emb_matrix_nn_w2v(file_path, vocab, d=300):\n",
    "    emb_dict = build_w2v_dict_nn(file_path, vocab, d=d)\n",
    "    emb_matrix = np.zeros((len(emb_dict), d))\n",
    "    word2idx = {'UNK': 0, 'PAD': 1}\n",
    "    for word in sorted(list(set(emb_dict.keys()) - set(['UNK', 'PAD']))):\n",
    "        word2idx[word] = len(word2idx)\n",
    "    for w, i in word2idx.items():\n",
    "        emb_matrix[i] = emb_dict[w]\n",
    "    emb_matrix = torch.tensor(emb_matrix)\n",
    "    return emb_matrix, word2idx\n",
    "\n",
    "class LR_Doc2Vec:\n",
    "    def __init__(self, doc2vec_model, C=1.0):\n",
    "        super(LR_Doc2Vec, self).__init__()\n",
    "        self.doc2vec_model = doc2vec_model\n",
    "        self.C = C\n",
    "        self.lr = LogisticRegression(C=C, random_state=13)\n",
    "    \n",
    "    def load_embeddings(self, X):\n",
    "        X_emb = []\n",
    "        for x in X:\n",
    "            X_emb.append(self.doc2vec_model.infer_vector(x))\n",
    "        X_emb = np.array(X_emb)\n",
    "        return X_emb\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train_emb = self.load_embeddings(X_train)\n",
    "        self.lr.fit(X_train_emb, y_train)\n",
    "        del X_train_emb\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_test_emb = self.load_embeddings(X_test)\n",
    "        y_pred = self.lr.predict(X_test_emb)\n",
    "        del X_test_emb\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        X_test_emb = self.load_embeddings(X_test)\n",
    "        y_pred = self.lr.predict_proba(X_test_emb)\n",
    "        del X_test_emb\n",
    "        return y_pred\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size=64, output_size=2, freeze_emb=True):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix)\n",
    "        if freeze_emb:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_emb = self.embedding(x)\n",
    "        # (batch, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(x_emb.float())\n",
    "        # (batch, seq_len, num_directions, hidden_size)\n",
    "        lstm_out = lstm_out.view(lstm_out.shape[0], lstm_out.shape[1], -1, self.hidden_size)\n",
    "        # lstm_out[:, :, 0, :] -- output of the forward LSTM\n",
    "        # lstm_out[:, :, 1, :] -- output of the backward LSTM\n",
    "        # we take the last hidden state of the forward LSTM and the first hidden state of the backward LSTM\n",
    "        x_fc = torch.cat((lstm_out[:, -1, 0, :], lstm_out[:, 0, 1, :]), dim=1)\n",
    "        fc_out = self.fc(x_fc)\n",
    "        out = self.softmax(fc_out)\n",
    "        return out\n",
    "\n",
    "def as_matrix(documents, word2idx, max_len=None):\n",
    "    max_doc_len = max(map(len, documents))\n",
    "    if max_len is None:\n",
    "        max_len = max_doc_len\n",
    "    else:\n",
    "        max_len = min(max_doc_len, max_len)\n",
    "    matrix = np.ones((len(documents), max_len), dtype=np.int64)\n",
    "    for i, doc in enumerate(documents):\n",
    "        row_ix = [word2idx.get(word, 0) for word in doc[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    return matrix\n",
    "\n",
    "def predict_bilstm(model, dev_data, word2idx, max_len=300, device=device, batch_size=16):\n",
    "    with torch.no_grad():\n",
    "        val_size = len(dev_data)\n",
    "        y_pred = np.zeros(val_size, dtype=float)\n",
    "        for i in range(0, val_size, batch_size):\n",
    "            x = as_matrix(dev_data[i:(i + batch_size)], word2idx, max_len)\n",
    "            x = torch.tensor(x).long()\n",
    "            x = x.to(device)\n",
    "            prediction = model(x)[:, 1]\n",
    "            y_pred[i:(i + batch_size)] = prediction.cpu().detach().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def set_random_seeds(seed_value=13, device='cpu'):\n",
    "    '''source https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/5'''\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    if device != 'cpu': \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), max_features=50000, min_df=5)\n",
    "train_texts_tfidf = tfidf_vec.fit_transform(train_texts)\n",
    "valid_texts_tfidf = tfidf_vec.transform(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7942782786624244\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=500, random_state=13)\n",
    "lr.fit(train_texts_tfidf, y_train)\n",
    "y_pred_lr_tfidf = lr.predict_proba(valid_texts_tfidf)[:, 1]\n",
    "print(roc_auc_score(y_valid, y_pred_lr_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3), max_features=40000, min_df=1)\n",
    "train_texts_cnt = cnt_vec.fit_transform(train_texts)\n",
    "valid_texts_cnt = cnt_vec.transform(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7877326249300394\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=500, random_state=13)\n",
    "lr.fit(train_texts_cnt, y_train)\n",
    "y_pred_lr_cnt = lr.predict_proba(valid_texts_cnt)[:, 1]\n",
    "print(roc_auc_score(y_valid, y_pred_lr_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 143374\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_tokens = tokenization(preprocessing(train_texts))\n",
    "valid_tokens = tokenization(preprocessing(valid_texts))\n",
    "\n",
    "vocab = build_vocabulary(train_tokens)\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vectors in embeddings dictionary: 78684\n",
      "Train embedding matrix shape: (758079, 300)\n",
      "Train: 1.34% unknown words\n",
      "Valid embedding matrix shape: (252694, 300)\n",
      "Valid: 1.59% unknown words\n"
     ]
    }
   ],
   "source": [
    "emb_dict = build_embeddings_glove('glove.6B.300d.txt', vocab)\n",
    "print('Unique vectors in embeddings dictionary:', len(emb_dict))\n",
    "\n",
    "train_emb_matrix, train_unk = build_emb_matrix_lr(train_tokens, emb_dict)\n",
    "valid_emb_matrix, valid_unk = build_emb_matrix_lr(valid_tokens, emb_dict)\n",
    "print('Train embedding matrix shape:', train_emb_matrix.shape)\n",
    "print('Train: {:.2f}% unknown words'.format(train_unk * 100))\n",
    "print('Valid embedding matrix shape:', valid_emb_matrix.shape)\n",
    "print('Valid: {:.2f}% unknown words'.format(valid_unk * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6748553399754051\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(C=5, solver='sag', max_iter=500, random_state=13)\n",
    "lr.fit(train_emb_matrix, y_train)\n",
    "y_pred_lr_glove = lr.predict_proba(valid_emb_matrix)[:, 1]\n",
    "print(roc_auc_score(y_valid, y_pred_lr_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vectors in embeddings dictionary: 62373\n",
      "Train embedding matrix shape: (758079, 300)\n",
      "Train: 23.67% unknown words\n",
      "Valid embedding matrix shape: (252694, 300)\n",
      "Valid: 23.71% unknown words\n"
     ]
    }
   ],
   "source": [
    "emb_dict = build_w2v_dict('googlenews/GoogleNews-vectors-negative300.bin', vocab)\n",
    "print('Unique vectors in embeddings dictionary:', len(emb_dict))\n",
    "\n",
    "train_emb_matrix, train_unk = build_emb_matrix_lr(train_tokens, emb_dict)\n",
    "valid_emb_matrix, valid_unk = build_emb_matrix_lr(valid_tokens, emb_dict)\n",
    "print('Train embedding matrix shape:', train_emb_matrix.shape)\n",
    "print('Train: {:.2f}% unknown words'.format(train_unk * 100))\n",
    "print('Valid embedding matrix shape:', valid_emb_matrix.shape)\n",
    "print('Valid: {:.2f}% unknown words'.format(valid_unk * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673213417887968\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=500, random_state=13)\n",
    "lr.fit(train_emb_matrix, y_train)\n",
    "y_pred_lr_w2v = lr.predict_proba(valid_emb_matrix)[:, 1]\n",
    "print(roc_auc_score(y_valid, y_pred_lr_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR + Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_docs = [TaggedDocument(d, [i]) for (i, d) in enumerate(train_tokens)]\n",
    "doc2vec_model = Doc2Vec(vector_size=300, min_count=1, epochs=5)\n",
    "doc2vec_model.build_vocab(train_docs)\n",
    "doc2vec_model.train(train_docs, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_doc2vec = LR_Doc2Vec(doc2vec_model)\n",
    "lr_doc2vec.fit(train_tokens, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6774603171532291\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred_lr_doc2vec = lr_doc2vec.predict_proba(valid_tokens)[:, 1]\n",
    "print(roc_auc_score(y_valid, y_pred_lr_doc2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 143374\n",
      "Unique vectors in embedding matrix: 78685\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "emb_matrix, word2idx = build_emb_matrix_nn('glove.6B.300d.txt', vocab)\n",
    "print('Unique vectors in embedding matrix:', len(emb_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8097815263231354\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BILSTM_STATE_DICT = 'bilstm_glove.pt'\n",
    "hidden_size = 128\n",
    "\n",
    "bilstm = BiLSTM(emb_matrix, hidden_size).to(device)\n",
    "bilstm.load_state_dict(torch.load(BILSTM_STATE_DICT, map_location=device))\n",
    "bilstm.eval()\n",
    "\n",
    "y_pred_bilstm_glove = predict_bilstm(bilstm, valid_tokens, word2idx)\n",
    "print(roc_auc_score(y_valid, y_pred_bilstm_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vectors in embedding matrix: 62374\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "emb_matrix, word2idx = build_emb_matrix_nn_w2v('googlenews/GoogleNews-vectors-negative300.bin', vocab)\n",
    "print('Unique vectors in embedding matrix:', len(emb_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7942717703722182\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BILSTM_STATE_DICT = 'bilstm_w2v.pt'\n",
    "hidden_size = 128\n",
    "\n",
    "bilstm = BiLSTM(emb_matrix, hidden_size).to(device)\n",
    "bilstm.load_state_dict(torch.load(BILSTM_STATE_DICT, map_location=device))\n",
    "bilstm.eval()\n",
    "\n",
    "y_pred_bilstm_w2v = predict_bilstm(bilstm, valid_tokens, word2idx)\n",
    "print(roc_auc_score(y_valid, y_pred_bilstm_w2v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
